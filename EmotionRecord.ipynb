{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0c2b129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguel/Desktop/CV/.venv/lib/python3.12/site-packages/face_recognition_models/__init__.py:7: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import datetime\n",
    "import time\n",
    "import face_recognition\n",
    "import os\n",
    "\n",
    "from src.model import MyModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dc79b6",
   "metadata": {},
   "source": [
    "## **CARGA DE DATOS**\n",
    "Creamos el transform para modificar la imagen para la entrada del modelo, el modelo entrenado y creamos una lista con las emociones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f15cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): LeakyReLU(negative_slope=0.01)\n",
       "    (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): LeakyReLU(negative_slope=0.01)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): LeakyReLU(negative_slope=0.01)\n",
       "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=8192, out_features=256, bias=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Linear(in_features=256, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = transforms.Compose([ # Transforms para convertir las imagenes a escala de grises a un tensor\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "EMOTIONS = [\"Angry\", \"Happy\", \"Neutral\", \"Sad\"] # Lista de emociones\n",
    "\n",
    "model = MyModel() # Cargamos nuestro modelo entrenado\n",
    "model.load_state_dict(torch.load(\"models/model.pth\", map_location=torch.device(\"cpu\")))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dffd1a2",
   "metadata": {},
   "source": [
    "## **INICIO**\n",
    "Para el funcionamiento del programa, hay que añadir las caras que queremos identificar en la carpeta \"./data/caras\", siendo el nombre del archivo \"nombre.*\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b97cc359",
   "metadata": {},
   "outputs": [],
   "source": [
    "knownFaces = []\n",
    "knownNames = []\n",
    "\n",
    "def iniciarCaras(carpeta): # Cargamos las caras identificadas de la ruta especificada\n",
    "    knownFaces.clear()\n",
    "    knownNames.clear()\n",
    "    for file in os.listdir(carpeta):\n",
    "        path = os.path.join(carpeta, file)\n",
    "\n",
    "        if os.path.isfile(path) and file.lower().endswith(('.jpg', '.jpeg', '.png', '.gif')): # Procesamos solo imagenes\n",
    "            name = os.path.splitext(file)[0].title()\n",
    "            try:\n",
    "                image = face_recognition.load_image_file(path)\n",
    "                encoding = face_recognition.face_encodings(image)[0]\n",
    "\n",
    "                if len(encoding) > 0:\n",
    "                    knownFaces.append(encoding)\n",
    "                    knownNames.append(name)\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(f\"{file}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f87eb022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardarDato(nombre, emocion): # Función para almacenar un nuevo dato en el dataframe\n",
    "    date = datetime.datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "    data = pd.DataFrame([{\n",
    "        \"Nombre\": nombre, \n",
    "        \"Fecha\": date,\n",
    "        \"Emoción\": emocion\n",
    "    }])\n",
    "\n",
    "    data.to_csv(\"resultados.csv\", mode='a', header=False, index=False)\n",
    "\n",
    "df = pd.DataFrame(columns=[\"Nombre\", \"Fecha\", \"Emoción\"]) # Creamos un dataframe para almacenar el historial\n",
    "df.to_csv(\"resultados.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ae87d4",
   "metadata": {},
   "source": [
    "## **CARGAR USUARIOS IDENTIFICADOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b9f3e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pedro']\n"
     ]
    }
   ],
   "source": [
    "carpeta = \"data/identidades\"\n",
    "iniciarCaras(carpeta) # Iniciamos las caras identificadas\n",
    "\n",
    "print(knownNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5afd7d8",
   "metadata": {},
   "source": [
    "## **RECONOCIMIENTO EN LOTE (FOTO)**\n",
    "Para su funcionamiento hay que añadir en la carpeta \"./data/input\" la imagen de una cara en formato \"nombrePersona.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "615145fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registrado con Nombre: Miguel, Fecha: 18-12-2025 19:55:19, Emoción: Neutral\n",
      "No hay más archivos\n"
     ]
    }
   ],
   "source": [
    "files = [f for f in os.listdir(\"data/input\") if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "for file in files:\n",
    "    path = os.path.join(\"data/input\", file) # Para cada imagen\n",
    "       \n",
    "    try:\n",
    "        image = face_recognition.load_image_file(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # Obtenemos la cara y lo pasamos a BGR\n",
    "        \n",
    "        faceLocations = face_recognition.face_locations(image, number_of_times_to_upsample=2) # Buscamos los pixeles de la cara\n",
    "        faceEncodings = face_recognition.face_encodings(image, faceLocations) # Y su codificación \n",
    "\n",
    "        for (top, right, bottom, left), faceEncoding in zip(faceLocations, faceEncodings): # Para cada cara de la imagen\n",
    "            matches = face_recognition.compare_faces(knownFaces, faceEncoding) # Comprobamos si existe ya en la carpeta identidades\n",
    "            name = \"Desconocido\"\n",
    "                \n",
    "            if True in matches: # Si existe obtenemos el nombre de la persona\n",
    "                firstMatch = matches.index(True)\n",
    "                name = knownNames[firstMatch]\n",
    "                \n",
    "            face = image[top:bottom, left:right] # Recortamos la cara\n",
    "            if face.size == 0: continue\n",
    "                \n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "            x = t(face).to(device).unsqueeze(0) # Procesamos la cara para el modelo\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y = model(x)\n",
    "                \n",
    "            prob = torch.nn.functional.softmax(y[0], dim=0) # Predecimos la salida aplicando softmax\n",
    "            pred = torch.argmax(prob).item() # Obtenemos el índice de la emoción\n",
    "            acc = prob[pred].item() # El accuracy de esa emoción\n",
    "            emotion = EMOTIONS[pred] # La emoción predicha\n",
    "\n",
    "            if name == \"Desconocido\": # Si no existe la persona la añadimos a nuestra carpeta de identidades\n",
    "                name = os.path.splitext(file)[0].title()\n",
    "                save_path = os.path.join(\"data/identidades\", f\"{name}.jpg\")\n",
    "                    \n",
    "                cv2.imwrite(save_path, image)\n",
    "                        \n",
    "                knownFaces.append(faceEncoding)\n",
    "                knownNames.append(name)\n",
    "                        \n",
    "            guardarDato(name, emotion) # Guardamos los datos en el csv\n",
    "            print(f\"Registrado con Nombre: {name}, Fecha: {datetime.datetime.now().strftime('%d-%m-%Y %H:%M:%S')}, Emoción: {emotion}\") # Imprimimos por pantalla el resultado\n",
    "                \n",
    "        if len(faceLocations) > 0:\n",
    "            os.remove(path) # Borramos de input los datos procesados\n",
    "        else:\n",
    "            print(f\"No se detectaron caras en {file}. No se borrará el archivo.\") # Si no hay caras avisamos\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{file}. Error: {e}\")\n",
    "\n",
    "print(\"No hay más archivos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a004bb9",
   "metadata": {},
   "source": [
    "## **RECONOCIMIENTO EN TIEMPO REAL (CÁMARA)**\n",
    "Para su funcionamiento basta con ejecutar la celda inferior. Para acabar la ejecución pulsa \"q\". Se puede modificar cada cuanto tiempo se almacena un dato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d88a1c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "TIEMPO = 5\n",
    "lastTime = time.time()\n",
    "inicio = time.time()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    currentTime = time.time()\n",
    "    \n",
    "    sframe = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25) # Reducimos el tamaño de la imagen para mayor rapidez (más eficiente)\n",
    "    sframe = cv2.cvtColor(sframe, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faceLocations = face_recognition.face_locations(sframe) # Buscamos los pixeles de la cara\n",
    "    faceEncodings = face_recognition.face_encodings(sframe, faceLocations) # Y su codificación \n",
    "\n",
    "    for (top, right, bottom, left), faceEncoding in zip(faceLocations, faceEncodings): # Para cada cara de la imagen\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4 # Reescalamos al tamaño original\n",
    "        \n",
    "        matches = face_recognition.compare_faces(knownFaces, faceEncoding) # Comprobamos si existe ya en la carpeta identidades\n",
    "        name = \"Desconocido\"\n",
    "        \n",
    "        if True in matches: # Si existe obtenemos el nombre de la persona\n",
    "            firstMatch = matches.index(True)\n",
    "            name = knownNames[firstMatch]\n",
    "        \n",
    "        face = frame[top:bottom, left:right] # Recortamos la cara\n",
    "        if face.size == 0: continue\n",
    "\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "        x = t(face).to(device).unsqueeze(0) # Procesamos la cara para el modelo\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y = model(x)\n",
    "            \n",
    "        prob = torch.nn.functional.softmax(y[0], dim=0) # Predecimos la salida aplicando softmax\n",
    "        pred = torch.argmax(prob).item() # Obtenemos el índice de la emoción\n",
    "        acc = prob[pred].item() # El accuracy de esa emoción\n",
    "        emotion = EMOTIONS[pred] # La emoción predicha\n",
    "\n",
    "        if name != \"Desconocido\" and (currentTime - lastTime) >= TIEMPO: # Almacenamos los resultados cada TIEMPO segundos si tenemos a la persona identificada\n",
    "            lastTime = currentTime\n",
    "            guardarDato(name, emotion)\n",
    "\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "        cv2.putText(frame, f\"{name}: {emotion}, {acc*100:.1f}%\", (left - 50, bottom + 50), cv2.FONT_HERSHEY_DUPLEX, 1.0, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow(\"Camara\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a12f774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
